{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":397693,"sourceType":"datasetVersion","datasetId":176381},{"sourceId":7312675,"sourceType":"datasetVersion","datasetId":2151340},{"sourceId":172160163,"sourceType":"kernelVersion"}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-28T19:09:55.685634Z","iopub.execute_input":"2024-05-28T19:09:55.686557Z","iopub.status.idle":"2024-05-28T19:09:58.533502Z","shell.execute_reply.started":"2024-05-28T19:09:55.686517Z","shell.execute_reply":"2024-05-28T19:09:58.532552Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport datetime as dt\nimport tensorflow as tf\nfrom collections import deque\n \nfrom sklearn.model_selection import train_test_split\n \nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model\n","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:09:58.535719Z","iopub.execute_input":"2024-05-28T19:09:58.536527Z","iopub.status.idle":"2024-05-28T19:10:09.819247Z","shell.execute_reply.started":"2024-05-28T19:09:58.536489Z","shell.execute_reply":"2024-05-28T19:10:09.818350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\nfrom base64 import b64encode\n\n# To Show a Video in Notebook\ndef Play_Video(filepath):\n    html = ''\n    video = open(filepath,'rb').read()\n    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n    html += '<video width=640 muted controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src \n    return HTML(html)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:10:09.822376Z","iopub.execute_input":"2024-05-28T19:10:09.823284Z","iopub.status.idle":"2024-05-28T19:10:09.828077Z","shell.execute_reply.started":"2024-05-28T19:10:09.823255Z","shell.execute_reply":"2024-05-28T19:10:09.827214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classes Directories\nNonViolnceVideos_Dir = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/\"\nViolnceVideos_Dir = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/\"\n\n# Retrieve the list of all the video files present in the Class Directory.\nNonViolence_files_names_list = os.listdir(NonViolnceVideos_Dir)\nViolence_files_names_list = os.listdir(ViolnceVideos_Dir)\n\n# Randomly select a video file from the Classes Directory.\nRandom_NonViolence_Video = random.choice(NonViolence_files_names_list)\nRandom_Violence_Video = random.choice(Violence_files_names_list)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:10:09.829124Z","iopub.execute_input":"2024-05-28T19:10:09.829399Z","iopub.status.idle":"2024-05-28T19:10:09.856291Z","shell.execute_reply.started":"2024-05-28T19:10:09.829375Z","shell.execute_reply":"2024-05-28T19:10:09.855660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Play_Video(f\"{NonViolnceVideos_Dir}/{Random_NonViolence_Video}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:10:09.858442Z","iopub.execute_input":"2024-05-28T19:10:09.858706Z","iopub.status.idle":"2024-05-28T19:10:09.896245Z","shell.execute_reply.started":"2024-05-28T19:10:09.858684Z","shell.execute_reply":"2024-05-28T19:10:09.895469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Play_Video(f\"{ViolnceVideos_Dir}/{Random_Violence_Video}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:10:09.897267Z","iopub.execute_input":"2024-05-28T19:10:09.897521Z","iopub.status.idle":"2024-05-28T19:10:09.998314Z","shell.execute_reply.started":"2024-05-28T19:10:09.897499Z","shell.execute_reply":"2024-05-28T19:10:09.997056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the height and width to which each video frame will be resized in our dataset.\nIMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n \n# Specify the number of frames of a video that will be fed to the model as one sequence.\nSEQUENCE_LENGTH = 16\n \n\nDATASET_DIR = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/\"\n \nCLASSES_LIST = [\"NonViolence\",\"Violence\"]","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:10:09.999607Z","iopub.execute_input":"2024-05-28T19:10:10.000748Z","iopub.status.idle":"2024-05-28T19:10:10.005020Z","shell.execute_reply.started":"2024-05-28T19:10:10.000721Z","shell.execute_reply":"2024-05-28T19:10:10.004143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def frames_extraction(video_path):\n \n    frames_list = []\n    \n    # Read the Video File\n    video_reader = cv2.VideoCapture(video_path)\n \n    # Get the total number of frames in the video.\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n \n    # Calculate the the interval after which frames will be added to the list.\n    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n \n    # Iterate through the Video Frames.\n    for frame_counter in range(SEQUENCE_LENGTH):\n \n        # Set the current frame position of the video.\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n \n        # Reading the frame from the video. \n        success, frame = video_reader.read() \n \n        if not success:\n            break\n \n        # Resize the Frame to fixed height and width.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        \n        # Normalize the resized frame\n        normalized_frame = resized_frame / 255\n        \n        # Append the normalized frame into the frames list\n        frames_list.append(normalized_frame)\n    \n \n    video_reader.release()\n \n    return frames_list","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:10:10.006188Z","iopub.execute_input":"2024-05-28T19:10:10.006448Z","iopub.status.idle":"2024-05-28T19:10:10.014327Z","shell.execute_reply.started":"2024-05-28T19:10:10.006425Z","shell.execute_reply":"2024-05-28T19:10:10.013390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataset():\n \n    features = []\n    labels = []\n    video_files_paths = []\n    \n    # Iterating through all the classes.\n    for class_index, class_name in enumerate(CLASSES_LIST):\n        \n        print(f'Extracting Data of Class: {class_name}')\n        \n        # Get the list of video files present in the specific class name directory.\n        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n        \n        # Iterate through all the files present in the files list.\n        for file_name in files_list:\n            \n            # Get the complete video path.\n            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n \n            # Extract the frames of the video file.\n            frames = frames_extraction(video_file_path)\n \n            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified.\n            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n            if len(frames) == SEQUENCE_LENGTH:\n \n                # Append the data to their repective lists.\n                features.append(frames)\n                labels.append(class_index)\n                video_files_paths.append(video_file_path)\n \n    features = np.asarray(features)\n    labels = np.array(labels)  \n\n    return features, labels, video_files_paths","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:10:10.015397Z","iopub.execute_input":"2024-05-28T19:10:10.015694Z","iopub.status.idle":"2024-05-28T19:10:10.025282Z","shell.execute_reply.started":"2024-05-28T19:10:10.015668Z","shell.execute_reply":"2024-05-28T19:10:10.024466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the dataset.\nfeatures, labels, video_files_paths = create_dataset()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:10:10.026356Z","iopub.execute_input":"2024-05-28T19:10:10.026639Z","iopub.status.idle":"2024-05-28T19:27:59.053768Z","shell.execute_reply.started":"2024-05-28T19:10:10.026616Z","shell.execute_reply":"2024-05-28T19:27:59.052950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the extracted data\nnp.save(\"features.npy\",features)\nnp.save(\"labels.npy\",labels)\nnp.save(\"video_files_paths.npy\",video_files_paths)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:27:59.054955Z","iopub.execute_input":"2024-05-28T19:27:59.055278Z","iopub.status.idle":"2024-05-28T19:28:01.265611Z","shell.execute_reply.started":"2024-05-28T19:27:59.055253Z","shell.execute_reply":"2024-05-28T19:28:01.264384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features, labels, video_files_paths = np.load(\"features.npy\") , np.load(\"labels.npy\") ,  np.load(\"video_files_paths.npy\")","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:28:01.266761Z","iopub.execute_input":"2024-05-28T19:28:01.267162Z","iopub.status.idle":"2024-05-28T19:28:02.302943Z","shell.execute_reply.started":"2024-05-28T19:28:01.267125Z","shell.execute_reply":"2024-05-28T19:28:02.301877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert labels into one-hot-encoded vectors\none_hot_encoded_labels = to_categorical(labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:28:02.304646Z","iopub.execute_input":"2024-05-28T19:28:02.305281Z","iopub.status.idle":"2024-05-28T19:28:02.310024Z","shell.execute_reply.started":"2024-05-28T19:28:02.305243Z","shell.execute_reply":"2024-05-28T19:28:02.309052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the Data into Train ( 90% ) and Test Set ( 10% ).\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.1,\n                                                                            shuffle = True, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:28:02.316624Z","iopub.execute_input":"2024-05-28T19:28:02.317422Z","iopub.status.idle":"2024-05-28T19:28:03.199295Z","shell.execute_reply.started":"2024-05-28T19:28:02.317391Z","shell.execute_reply":"2024-05-28T19:28:03.198305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:28:03.200598Z","iopub.execute_input":"2024-05-28T19:28:03.201256Z","iopub.status.idle":"2024-05-28T19:29:52.969864Z","shell.execute_reply.started":"2024-05-28T19:28:03.201218Z","shell.execute_reply":"2024-05-28T19:29:52.968760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(features_train.shape,labels_train.shape )\nprint(features_test.shape, labels_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:29:52.971621Z","iopub.execute_input":"2024-05-28T19:29:52.972040Z","iopub.status.idle":"2024-05-28T19:29:52.978371Z","shell.execute_reply.started":"2024-05-28T19:29:52.972001Z","shell.execute_reply":"2024-05-28T19:29:52.977311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.applications.mobilenet_v2 import MobileNetV2\n\nmobilenet = MobileNetV2( include_top=False , weights=\"imagenet\")\n\n#Fine-Tuning to make the last 40 layer trainable\nmobilenet.trainable=True\n\nfor layer in mobilenet.layers[:-40]:\n  layer.trainable=False\n\n#mobilenet.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:29:52.979696Z","iopub.execute_input":"2024-05-28T19:29:52.980113Z","iopub.status.idle":"2024-05-28T19:29:56.231191Z","shell.execute_reply.started":"2024-05-28T19:29:52.980057Z","shell.execute_reply":"2024-05-28T19:29:56.230254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model():\n \n    model = Sequential()\n\n    ########################################################################################################################\n    \n    #Specifying Input to match features shape\n    model.add(Input(shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n    \n    # Passing mobilenet in the TimeDistributed layer to handle the sequence\n    model.add(TimeDistributed(mobilenet))\n    \n    model.add(Dropout(0.25))\n                                    \n    model.add(TimeDistributed(Flatten()))\n\n    \n    lstm_fw = LSTM(units=32)\n    lstm_bw = LSTM(units=32, go_backwards = True)  \n\n    model.add(Bidirectional(lstm_fw, backward_layer = lstm_bw))\n    \n    model.add(Dropout(0.25))\n\n    model.add(Dense(256,activation='relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Dense(128,activation='relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Dense(64,activation='relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Dense(32,activation='relu'))\n    model.add(Dropout(0.25))\n    \n    \n    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n \n    ########################################################################################################################\n \n    model.summary()\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:29:56.232527Z","iopub.execute_input":"2024-05-28T19:29:56.232911Z","iopub.status.idle":"2024-05-28T19:29:56.242400Z","shell.execute_reply.started":"2024-05-28T19:29:56.232877Z","shell.execute_reply":"2024-05-28T19:29:56.241462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constructing the Model\nMoBiLSTM_model = create_model()\n\n# Plot the structure of the contructed LRCN model.\nplot_model(MoBiLSTM_model, to_file = 'MobBiLSTM_model_structure_plot.png', show_shapes = True, show_layer_names = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:29:56.243625Z","iopub.execute_input":"2024-05-28T19:29:56.243906Z","iopub.status.idle":"2024-05-28T19:29:57.988224Z","shell.execute_reply.started":"2024-05-28T19:29:56.243882Z","shell.execute_reply":"2024-05-28T19:29:57.987326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:29:57.989476Z","iopub.execute_input":"2024-05-28T19:29:57.989825Z","iopub.status.idle":"2024-05-28T19:29:57.994285Z","shell.execute_reply.started":"2024-05-28T19:29:57.989798Z","shell.execute_reply":"2024-05-28T19:29:57.993406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Early Stopping Callback to monitor the accuracy\nearly_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = 10, restore_best_weights = True)\n\n# Create ReduceLROnPlateau Callback to reduce overfitting by decreasing learning\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                  factor=0.6,\n                                                  patience=5,\n                                                  min_lr=0.00005,\n                                                  verbose=1)\n \n# Compiling the model \nMoBiLSTM_model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd', metrics = [\"accuracy\"])\n \n# Fitting the model \nMobBiLSTM_model_history = MoBiLSTM_model.fit(x = features_train, y = labels_train, epochs = 80, batch_size = 8 ,\n                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback,reduce_lr])","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:29:57.995412Z","iopub.execute_input":"2024-05-28T19:29:57.995707Z","iopub.status.idle":"2024-05-28T19:35:29.137667Z","shell.execute_reply.started":"2024-05-28T19:29:57.995683Z","shell.execute_reply":"2024-05-28T19:35:29.136881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation_history = MoBiLSTM_model.evaluate(features_test, labels_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:35:29.138823Z","iopub.execute_input":"2024-05-28T19:35:29.139144Z","iopub.status.idle":"2024-05-28T19:35:34.076435Z","shell.execute_reply.started":"2024-05-28T19:35:29.139117Z","shell.execute_reply":"2024-05-28T19:35:34.075702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n    \n    metric_value_1 = model_training_history.history[metric_name_1]\n    metric_value_2 = model_training_history.history[metric_name_2]\n    \n    # Get the Epochs Count\n    epochs = range(len(metric_value_1))\n \n    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n    plt.plot(epochs, metric_value_2, 'orange', label = metric_name_2)\n \n    plt.title(str(plot_name))\n \n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:35:34.077798Z","iopub.execute_input":"2024-05-28T19:35:34.078096Z","iopub.status.idle":"2024-05-28T19:35:34.089447Z","shell.execute_reply.started":"2024-05-28T19:35:34.078070Z","shell.execute_reply":"2024-05-28T19:35:34.088546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(MobBiLSTM_model_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:37:53.205438Z","iopub.execute_input":"2024-05-28T19:37:53.205851Z","iopub.status.idle":"2024-05-28T19:37:53.262351Z","shell.execute_reply.started":"2024-05-28T19:37:53.205823Z","shell.execute_reply":"2024-05-28T19:37:53.261161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(MobBiLSTM_model_history, 'accuracy', 'val_accuracy', 'Total Loss vs Total Validation Loss')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:35:34.522032Z","iopub.status.idle":"2024-05-28T19:35:34.522372Z","shell.execute_reply.started":"2024-05-28T19:35:34.522191Z","shell.execute_reply":"2024-05-28T19:35:34.522206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_predict = MoBiLSTM_model.predict(features_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:37:57.299528Z","iopub.execute_input":"2024-05-28T19:37:57.300661Z","iopub.status.idle":"2024-05-28T19:38:00.054524Z","shell.execute_reply.started":"2024-05-28T19:37:57.300629Z","shell.execute_reply":"2024-05-28T19:38:00.053719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decoding the data to use in Metrics\nlabels_predict = np.argmax(labels_predict , axis=1)\nlabels_test_normal = np.argmax(labels_test , axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:38:05.943963Z","iopub.execute_input":"2024-05-28T19:38:05.944312Z","iopub.status.idle":"2024-05-28T19:38:05.949771Z","shell.execute_reply.started":"2024-05-28T19:38:05.944284Z","shell.execute_reply":"2024-05-28T19:38:05.948463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_test_normal.shape , labels_predict.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:38:17.423367Z","iopub.execute_input":"2024-05-28T19:38:17.424212Z","iopub.status.idle":"2024-05-28T19:38:17.429878Z","shell.execute_reply.started":"2024-05-28T19:38:17.424177Z","shell.execute_reply":"2024-05-28T19:38:17.428933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nAccScore = accuracy_score(labels_predict, labels_test_normal)\nprint('Accuracy Score is : ', AccScore)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:38:20.387260Z","iopub.execute_input":"2024-05-28T19:38:20.388118Z","iopub.status.idle":"2024-05-28T19:38:20.394337Z","shell.execute_reply.started":"2024-05-28T19:38:20.388087Z","shell.execute_reply":"2024-05-28T19:38:20.393413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns \nfrom sklearn.metrics import confusion_matrix\n\nax= plt.subplot()\ncm=confusion_matrix(labels_test_normal, labels_predict)\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  \n\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['True', 'False']); ax.yaxis.set_ticklabels(['NonViolence', 'Violence']);","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:38:22.702599Z","iopub.execute_input":"2024-05-28T19:38:22.702963Z","iopub.status.idle":"2024-05-28T19:38:22.903783Z","shell.execute_reply.started":"2024-05-28T19:38:22.702927Z","shell.execute_reply":"2024-05-28T19:38:22.902546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nClassificationReport = classification_report(labels_test_normal,labels_predict)\nprint('Classification Report is : \\n', ClassificationReport)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:38:26.850123Z","iopub.execute_input":"2024-05-28T19:38:26.850449Z","iopub.status.idle":"2024-05-28T19:38:26.866022Z","shell.execute_reply.started":"2024-05-28T19:38:26.850424Z","shell.execute_reply":"2024-05-28T19:38:26.865255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_frames(video_file_path, output_file_path, SEQUENCE_LENGTH):\n    \n    # Read from the video file.\n    video_reader = cv2.VideoCapture(video_file_path)\n \n    # Get the width and height of the video.\n    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n \n    # VideoWriter to store the output video in the disk.\n    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), \n                                    video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n \n    # Declare a queue to store video frames.\n    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n \n    # Store the predicted class in the video.\n    predicted_class_name = ''\n \n    # Iterate until the video is accessed successfully.\n    while video_reader.isOpened():\n \n        ok, frame = video_reader.read() \n        \n        if not ok:\n            break\n \n        # Resize the Frame to fixed Dimensions.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        \n        # Normalize the resized frame \n        normalized_frame = resized_frame / 255\n \n        # Appending the pre-processed frame into the frames list.\n        frames_queue.append(normalized_frame)\n \n        # We Need at Least number of SEQUENCE_LENGTH Frames to perform a prediction.\n        # Check if the number of frames in the queue are equal to the fixed sequence length.\n        if len(frames_queue) == SEQUENCE_LENGTH:                        \n \n            # Pass the normalized frames to the model and get the predicted probabilities.\n            predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_queue, axis = 0))[0]\n \n            # Get the index of class with highest probability.\n            predicted_label = np.argmax(predicted_labels_probabilities)\n \n            # Get the class name using the retrieved index.\n            predicted_class_name = CLASSES_LIST[predicted_label]\n \n        # Write predicted class name on top of the frame.\n        if predicted_class_name == \"Violence\":\n            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 12)\n        else:\n            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 12)\n         \n        # Write The frame into the disk using the VideoWriter\n        video_writer.write(frame)                       \n        \n    video_reader.release()\n    video_writer.release()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:38:31.457441Z","iopub.execute_input":"2024-05-28T19:38:31.457824Z","iopub.status.idle":"2024-05-28T19:38:31.470507Z","shell.execute_reply.started":"2024-05-28T19:38:31.457794Z","shell.execute_reply":"2024-05-28T19:38:31.469147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:40:51.149769Z","iopub.execute_input":"2024-05-28T19:40:51.150173Z","iopub.status.idle":"2024-05-28T19:40:51.154577Z","shell.execute_reply.started":"2024-05-28T19:40:51.150141Z","shell.execute_reply":"2024-05-28T19:40:51.153633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"default\")\n\n'''# To show Random Frames from the saved output predicted video (output predicted video doesn't show on the notebook but can be downloaded)\ndef show_pred_frames(pred_video_path): \n\n    plt.figure(figsize=(20,15))\n\n    video_reader = cv2.VideoCapture(pred_video_path)\n\n    # Get the number of frames in the video.\n    frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    # Get Random Frames from the video then Sort it\n    random_range = sorted(random.sample(range (SEQUENCE_LENGTH , frames_count ), 12))\n        \n    for counter, random_index in enumerate(random_range, 1):\n        \n        plt.subplot(5, 4, counter)\n\n        # Set the current frame position of the video.  \n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, random_index)\n          \n        ok, frame = video_reader.read() \n\n        if not ok:\n          break \n\n        frame = cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)\n\n        plt.imshow(frame);ax.figure.set_size_inches(20,20);plt.tight_layout()\n                            \n    video_reader.release()'''\n\n\n# To show Random Frames from the saved output predicted video (output predicted video doesn't show on the notebook but can be downloaded)\ndef show_pred_frames(pred_video_path): \n\n    plt.figure(figsize=(20,15))\n\n    video_reader = cv2.VideoCapture(pred_video_path)\n\n    # Get the number of frames in the video.\n    frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    # Get Random Frames from the video then Sort it\n    random_range = sorted(random.sample(range (SEQUENCE_LENGTH , frames_count ), 12))\n        \n    for counter, random_index in enumerate(random_range, 1):\n        \n        plt.subplot(5, 4, counter)\n\n        # Set the current frame position of the video.  \n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, random_index)\n          \n        ok, frame = video_reader.read() \n\n        if not ok:\n          break \n\n        frame = cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)\n\n        plt.imshow(frame)\n                            \n    video_reader.release()\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:43:01.197453Z","iopub.execute_input":"2024-05-28T19:43:01.197861Z","iopub.status.idle":"2024-05-28T19:43:01.208806Z","shell.execute_reply.started":"2024-05-28T19:43:01.197832Z","shell.execute_reply":"2024-05-28T19:43:01.207818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Construct the output video path.\ntest_videos_directory = 'test_videos'\nos.makedirs(test_videos_directory, exist_ok = True)\n \noutput_video_file_path = f'{test_videos_directory}/Output-Test-Video.mp4'","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:43:06.600523Z","iopub.execute_input":"2024-05-28T19:43:06.601202Z","iopub.status.idle":"2024-05-28T19:43:06.605484Z","shell.execute_reply.started":"2024-05-28T19:43:06.601168Z","shell.execute_reply":"2024-05-28T19:43:06.604547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specifying video to be predicted\ninput_video_file_path = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/V_378.mp4\"\n\n# Perform Prediction on the Test Video.\npredict_frames(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n\n# Show random frames from the output video\nshow_pred_frames(output_video_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:43:08.593497Z","iopub.execute_input":"2024-05-28T19:43:08.594273Z","iopub.status.idle":"2024-05-28T19:43:19.690114Z","shell.execute_reply.started":"2024-05-28T19:43:08.594234Z","shell.execute_reply":"2024-05-28T19:43:19.689145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Play the actual video\nPlay_Video(input_video_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:39:09.119801Z","iopub.execute_input":"2024-05-28T19:39:09.120134Z","iopub.status.idle":"2024-05-28T19:39:09.145130Z","shell.execute_reply.started":"2024-05-28T19:39:09.120109Z","shell.execute_reply":"2024-05-28T19:39:09.144261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specifying video to be predicted\ninput_video_file_path = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/NV_1.mp4\"\n\n# Perform Prediction on the Test Video.\npredict_frames(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n\n# Show random frames from the output video\nshow_pred_frames(output_video_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:43:25.676398Z","iopub.execute_input":"2024-05-28T19:43:25.677227Z","iopub.status.idle":"2024-05-28T19:43:38.602683Z","shell.execute_reply.started":"2024-05-28T19:43:25.677192Z","shell.execute_reply":"2024-05-28T19:43:38.601662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Play the actual video\nPlay_Video(input_video_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:35:34.548340Z","iopub.status.idle":"2024-05-28T19:35:34.548785Z","shell.execute_reply.started":"2024-05-28T19:35:34.548545Z","shell.execute_reply":"2024-05-28T19:35:34.548581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_video(video_file_path, SEQUENCE_LENGTH):\n \n    video_reader = cv2.VideoCapture(video_file_path)\n \n    # Get the width and height of the video.\n    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n \n    # Declare a list to store video frames we will extract.\n    frames_list = []\n    \n    # Store the predicted class in the video.\n    predicted_class_name = ''\n \n    # Get the number of frames in the video.\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n \n    # Calculate the interval after which frames will be added to the list.\n    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH),1)\n \n    # Iterating the number of times equal to the fixed length of sequence.\n    for frame_counter in range(SEQUENCE_LENGTH):\n \n        # Set the current frame position of the video.\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n \n        success, frame = video_reader.read() \n \n        if not success:\n            break\n \n        # Resize the Frame to fixed Dimensions.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        \n        # Normalize the resized frame.\n        normalized_frame = resized_frame / 255\n        \n        # Appending the pre-processed frame into the frames list\n        frames_list.append(normalized_frame)\n \n    # Passing the  pre-processed frames to the model and get the predicted probabilities.\n    predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_list, axis = 0))[0]\n \n    # Get the index of class with highest probability.\n    predicted_label = np.argmax(predicted_labels_probabilities)\n \n    # Get the class name using the retrieved index.\n    predicted_class_name = CLASSES_LIST[predicted_label]\n    \n    # Display the predicted class along with the prediction confidence.\n    print(f'Predicted: {predicted_class_name}\\nConfidence: {predicted_labels_probabilities[predicted_label]}')\n        \n    video_reader.release()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:43:49.355194Z","iopub.execute_input":"2024-05-28T19:43:49.356059Z","iopub.status.idle":"2024-05-28T19:43:49.365313Z","shell.execute_reply.started":"2024-05-28T19:43:49.356024Z","shell.execute_reply":"2024-05-28T19:43:49.364318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specifying video to be predicted\ninput_video_file_path = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/V_276.mp4\"\n\n# Perform Single Prediction on the Test Video.\npredict_video(input_video_file_path, SEQUENCE_LENGTH)\n\n# Play the actual video\nPlay_Video(input_video_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:43:53.650407Z","iopub.execute_input":"2024-05-28T19:43:53.651254Z","iopub.status.idle":"2024-05-28T19:43:55.984645Z","shell.execute_reply.started":"2024-05-28T19:43:53.651218Z","shell.execute_reply":"2024-05-28T19:43:55.982109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specifying video to be predicted\ninput_video_file_path = \"../input/real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/NV_23.mp4\"\n\n# Perform Single Prediction on the Test Video.\npredict_video(input_video_file_path, SEQUENCE_LENGTH)\n\n# Play the actual video\nPlay_Video(input_video_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:39:36.651616Z","iopub.execute_input":"2024-05-28T19:39:36.651946Z","iopub.status.idle":"2024-05-28T19:39:38.666444Z","shell.execute_reply.started":"2024-05-28T19:39:36.651921Z","shell.execute_reply":"2024-05-28T19:39:38.665537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args_model = \"mobilnetLstm.h5\"\nMoBiLSTM_model.save(args_model)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T19:44:20.748349Z","iopub.execute_input":"2024-05-28T19:44:20.748715Z","iopub.status.idle":"2024-05-28T19:44:21.085071Z","shell.execute_reply.started":"2024-05-28T19:44:20.748685Z","shell.execute_reply":"2024-05-28T19:44:21.084073Z"},"trusted":true},"execution_count":null,"outputs":[]}]}